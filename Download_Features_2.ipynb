{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hkris\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import pandas as pd\n",
    "import math\n",
    "from operator import itemgetter\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting given dates to milisec to be able to query blocks from blockchain.info by dates\n",
    "def dates_to_milisec(start,end):\n",
    "\n",
    "    if start == end:\n",
    "        days=1\n",
    "    else:\n",
    "        date_start = datetime.datetime.strptime(str(start),'%Y%m%d')\n",
    "        date_end = datetime.datetime.strptime(str(end), '%Y%m%d')\n",
    "        days = (date_end-date_start).days\n",
    "\n",
    "    date_list = [date_start + datetime.timedelta(days=x) for x in range(0,days+1)]\n",
    "    #date_list_in_ms = [date_list[x].timestamp() * 1000 for x in range(len(date_list))]\n",
    "    date_list_in_ms = [time.mktime(date_list[x].timetuple()) * 1000.0 for x in range(len(date_list))]\n",
    "    \n",
    "    return date_list_in_ms\n",
    "\n",
    "\n",
    "\n",
    "#for each day there is a different array containing the blocks for that day\n",
    "def query_block_hashes(date_list_in_ms):\n",
    "\n",
    "    blocks = []\n",
    "\n",
    "    for i in range(len(date_list_in_ms)):\n",
    "        r = requests.get(\"https://blockchain.info/blocks/\" + str(int(date_list_in_ms[i])) + \"?format=json\")\n",
    "        data = r.json()\n",
    "        blocks.append(data)\n",
    "\n",
    "    block_hashes = []\n",
    "\n",
    "    for i in range(len(blocks)):\n",
    "\n",
    "        hashes_ = [blocks[i]['blocks'][n]['hash'] for n in range(len(blocks[i]['blocks']))]\n",
    "        block_hashes.append(hashes_)\n",
    "\n",
    "    return block_hashes\n",
    "\n",
    "\n",
    "\n",
    "#BTC block parser\n",
    "def parse_block_data(block_hash):\n",
    "\n",
    "\n",
    "    r = requests.get(\"https://blockchain.info/rawblock/\" + block_hash)\n",
    "    data = r.json()\n",
    "\n",
    "    given_block_data = {}\n",
    "    block_height = int(data[\"height\"])\n",
    "    block_hash_ = block_hash\n",
    "    \n",
    "    creation_time = str(datetime.datetime.utcfromtimestamp(float(data[\"time\"])))   # -1 hour for my time zone\n",
    "    #other metadata to store...\n",
    "\n",
    "    for i in range(0,len(data[\"tx\"])):\n",
    "\n",
    "        tr_hash = data[\"tx\"][i][\"hash\"]\n",
    "        inputs = []\n",
    "        inputs_values = []\n",
    "        for k in range(len(data[\"tx\"][i][\"inputs\"])):\n",
    "            if \"prev_out\" in data[\"tx\"][i][\"inputs\"][k]:\n",
    "                inputs += [data[\"tx\"][i][\"inputs\"][k][\"prev_out\"][\"addr\"]]\n",
    "                inputs_values += [data[\"tx\"][i][\"inputs\"][k][\"prev_out\"][\"value\"]]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        outputs = []\n",
    "        outputs_values = []\n",
    "        for k in range(len(data[\"tx\"][i][\"out\"])):\n",
    "\n",
    "            if \"addr\" in data[\"tx\"][i][\"out\"][k]:\n",
    "                outputs += [data[\"tx\"][i][\"out\"][k][\"addr\"]]\n",
    "                outputs_values += [data[\"tx\"][i][\"out\"][k][\"value\"]]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        given_block_data[tr_hash] = [(inputs,inputs_values),(outputs,outputs_values)]\n",
    "\n",
    "    return given_block_data,block_height,creation_time, block_hash_\n",
    "\n",
    "\n",
    "\n",
    "#create networkx graph from one block's transactions. Because BTC input and output transactions can't be directly assigned to each other,\n",
    "#an auxiliary node is drawed for every transactions. This auxiliary node represents the given transaction's hash. Input transactions run in, and\n",
    "#output transactions arise from this auxiliary node.\n",
    "def create_tr_graph(given_block_data):\n",
    "\n",
    "        DG = nx.MultiDiGraph()\n",
    "\n",
    "        for key, value in given_block_data.items():\n",
    "\n",
    "            tr_hash = key\n",
    "            for i in range(len(given_block_data[tr_hash][0][0])):\n",
    "                DG.add_edges_from([(given_block_data[tr_hash][0][0][i], tr_hash)], weight = given_block_data[tr_hash][0][1][i] / 10**8)\n",
    "\n",
    "            for i in range(len(given_block_data[tr_hash][1][0])):\n",
    "                DG.add_edges_from([(tr_hash, given_block_data[tr_hash][1][0][i])], weight = given_block_data[tr_hash][1][1][i] / 10**8)\n",
    "\n",
    "        return DG\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#generate list of dates in str format    \n",
    "def generate_date_list(start,end):\n",
    "\n",
    "    if start == end:\n",
    "        days=1\n",
    "    else:\n",
    "    \n",
    "        date_start = datetime.datetime.strptime(str(start),'%Y%m%d')\n",
    "        date_end = datetime.datetime.strptime(str(end), '%Y%m%d')\n",
    "        days = (date_end-date_start).days\n",
    "        date_list = [date_start + datetime.timedelta(days=x) for x in range(0,days+1)]\n",
    "        date_list = [x.strftime('%Y-%m-%d') for x in date_list]\n",
    "    \n",
    "    return date_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create hdf5 file from blocks transaction matrices (adjacency matrices) with corresponding meta data and a transaction graph picture for each block\n",
    "#block heights identify the given block's transaction matrix and the corresponding graph picture. Transactions graph can not be recovered as networkx\n",
    "#multidigraph from the adjacency matrices!!! For analysing each block's directed transaction graphs the create_tr_graph function should be used!\n",
    "def append_to_hdf5_file(matrix,block_height,block_creation_time, block_hash):\n",
    "\n",
    "\n",
    "    hdf5_file = h5py.File('correlating_datasets_test.hdf5', mode=\"a\")\n",
    "\n",
    "    if 'transaction_matrices' not in list(hdf5_file.keys()):\n",
    "        grp = hdf5_file.create_group(\"transaction_matrices\")\n",
    "\n",
    "    else:\n",
    "        grp = hdf5_file['transaction_matrices']\n",
    "\n",
    "\n",
    "    if str(block_height) not in list(hdf5_file['transaction_matrices'].keys()):\n",
    "\n",
    "        dataset = grp.create_dataset(str(block_height),data=matrix,\n",
    "                                     dtype=np.uint8,compression='gzip',shuffle=True)\n",
    "\n",
    "        dataset.attrs['creation_time'] = block_creation_time\n",
    "        dataset.attrs['block_height'] = block_height\n",
    "        dataset.attrs['block_hash'] = block_hash\n",
    "        #other metadata for each block's metadata...\n",
    "        #image = mpimg.imread(os.getcwd() + \"\\\\\" + str(block_height) + \".png\")\n",
    "        #dataset = grp.create_dataset(str(block_height) + \"_picture\", data=image)\n",
    "\n",
    "\n",
    "    hdf5_file.close()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#Finding a correspondig volatility value for a given block creation time. The volatility value is the nearest in a given\n",
    "#time interval. Lets say the volatility is calculated in a minutely sampled price data with a five long window. The\n",
    "#time interval is 10 minutes, so the volatility value will be the closest to the block creation time in a 10 minute \n",
    "#interval.\n",
    "\n",
    "def find_corresponding_volatility_and_price(df, creation_time):\n",
    "    \n",
    "    creation_time = datetime.datetime.strptime(creation_time, '%Y-%m-%d %H:%M:%S')\n",
    "    df_ = df[creation_time : creation_time + datetime.timedelta(minutes=10)]\n",
    "    df_ = df_.iloc[df_.index.get_loc(creation_time,method='nearest')]\n",
    "    \n",
    "    return df_['BVOL5M_INDEX'], df_['Weighted_Price']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def append_to_hdf5_file_with_price_and_volatility(matrix,block_height,block_creation_time,block_hash,df_price_vol):\n",
    "\n",
    "    volatility = 0\n",
    "    \n",
    "    hdf5_file = h5py.File('correlating_datasets_test.hdf5', mode=\"a\")\n",
    "\n",
    "    if 'transaction_matrices' not in list(hdf5_file.keys()):\n",
    "        grp = hdf5_file.create_group(\"transaction_matrices\")\n",
    "\n",
    "    else:\n",
    "        grp = hdf5_file['transaction_matrices']\n",
    "\n",
    "\n",
    "    if str(block_height) not in list(hdf5_file['transaction_matrices'].keys()):\n",
    "\n",
    "        dataset = grp.create_dataset(str(block_height),data=matrix,\n",
    "                                     dtype=np.uint8,compression='gzip',shuffle=True)\n",
    "\n",
    "        \n",
    "        volatility, price = find_corresponding_volatility_and_price(df_price_vol, block_creation_time)\n",
    "        \n",
    "        dataset.attrs['creation_time'] = block_creation_time\n",
    "        dataset.attrs['block_height'] = block_height\n",
    "        dataset.attrs['BVOL5M_INDEX'] = volatility\n",
    "        dataset.attrs['Weighted_Price'] = price\n",
    "        dataset.attrs['block_hash'] = block_hash\n",
    "        \n",
    "        \n",
    "        #other metadata for each block's metadata...\n",
    "        #image = mpimg.imread(os.getcwd() + \"\\\\\" + str(block_height) + \".png\")\n",
    "        #dataset = grp.create_dataset(str(block_height) + \"_picture\", data=image)\n",
    "\n",
    "\n",
    "    hdf5_file.close()\n",
    "    \n",
    "    return volatility\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#create subsets with equal sizes. If sizes mismatch, there will be an error when calculating correlation\n",
    "\n",
    "def create_subsets(dataframe, length_of_subsets):\n",
    "    \n",
    "    n = length_of_subsets\n",
    "    res = len(dataframe)%n\n",
    "    df_ = dataframe[:-res]   \n",
    "    list_df = [df_['BVOL5M_INDEX'][i:i+n] for i in range(0, df_.shape[0], n)]\n",
    "    \n",
    "    return list_df\n",
    "\n",
    "\n",
    "def create_time_subsets(dataframe, length_of_subsets):\n",
    "    \n",
    "    n = length_of_subsets\n",
    "    res = len(dataframe)%n\n",
    "    df_ = dataframe[:-res]   \n",
    "    list_df = [df_['Unnamed: 0'][i:i+n] for i in range(0, df_.shape[0], n)]\n",
    "    \n",
    "    return list_df\n",
    "\n",
    "\n",
    "'''\n",
    "Function to create a DataFrame containing column_name - value pairs where value is the number of correlating subsets in \n",
    "each column.\n",
    "'''\n",
    "\n",
    "def find_longest_correlating_subsets(df_correlating_subsets):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for column in df_correlating_subsets.columns:\n",
    "\n",
    "        length = 0\n",
    "\n",
    "        for index, row in df_correlating_subsets[[column]].iterrows():\n",
    "\n",
    "            if pd.isnull(row.values):\n",
    "\n",
    "                df_ = pd.DataFrame(columns=[column], data=[length])\n",
    "                df = pd.concat([df,df_], axis=1)\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                length+=1\n",
    "            \n",
    "    return df\n",
    "\n",
    "\n",
    "'''\n",
    "Function to find the columns where the number of correlating subsets exceed a given threshold. For example if threshold is\n",
    "30, the column must contain 30 correlating volatility subsets.\n",
    "'''\n",
    "\n",
    "def find_columns_that_contain_enough_correlations(df_number_of_correlations, threshold):\n",
    "    \n",
    "    l = []\n",
    "\n",
    "    for i in df_number_of_correlations.columns:\n",
    "\n",
    "        if df_number_of_correlations[i].values > threshold:\n",
    "\n",
    "            l.append(i)\n",
    "            \n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_in_ms = dates_to_milisec(20180329, 20180430)\n",
    "block_hashes = query_block_hashes(dates_in_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day:  0\n",
      "day:  1\n",
      "day:  2\n",
      "day:  3\n",
      "day:  4\n",
      "day:  5\n",
      "day:  6\n",
      "day:  7\n",
      "day:  8\n",
      "day:  9\n",
      "day:  10\n",
      "day:  11\n",
      "day:  12\n",
      "day:  13\n",
      "day:  14\n",
      "day:  15\n",
      "day:  16\n",
      "day:  17\n",
      "day:  18\n",
      "day:  19\n",
      "day:  20\n",
      "day:  21\n",
      "day:  22\n",
      "day:  23\n",
      "day:  24\n",
      "day:  25\n",
      "day:  26\n",
      "day:  27\n",
      "day:  28\n",
      "day:  29\n",
      "day:  30\n",
      "day:  31\n",
      "day:  32\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "for k in range(len(block_hashes)):\n",
    "    \n",
    "    print('day: ',k)\n",
    "    \n",
    "    for n in range(len(block_hashes[k])):\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            block_height = []\n",
    "            creation_time = []\n",
    "            tx_number = []\n",
    "            block_size = []\n",
    "            nonce = []\n",
    "            block_hash = []\n",
    "            avg_tr_sizes = []\n",
    "            sum_tr_sizes = []\n",
    "            mining_fee = []\n",
    "            all_reward = []\n",
    "            diff_target = []\n",
    "            total_btc_output_in_block = []\n",
    "            \n",
    "            r = requests.get(\"https://blockchain.info/rawblock/\" + block_hashes[k][n])\n",
    "            data = r.json()\n",
    "\n",
    "            block_height.append(data['height'])\n",
    "            creation_time.append(str(datetime.datetime.utcfromtimestamp(float(data[\"time\"]))))\n",
    "            tx_number.append(data['n_tx'])\n",
    "            block_size.append(data['size'])\n",
    "            nonce.append(data['nonce'])\n",
    "            block_hash.append(data['hash'])\n",
    "\n",
    "            avg_tr_size = 0\n",
    "            sum_tr_size = 0\n",
    "            for i in range(len(data['tx'])):\n",
    "                sum_tr_size += data['tx'][i]['size']\n",
    "                avg_tr_size = sum_tr_size / len(data['tx'])\n",
    "\n",
    "            avg_tr_sizes.append(avg_tr_size)\n",
    "            sum_tr_sizes.append(sum_tr_size)\n",
    "\n",
    "            mining_fee.append(data['tx'][0]['out'][0]['value'] / pow(10,8) - 12.5)\n",
    "            all_reward.append(data['tx'][0]['out'][0]['value'] / pow(10,8))\n",
    "            #kell még az USD értékük is majd...\n",
    "\n",
    "            #Difficulty in decimal\n",
    "            #target = coefficient*2^(8*(exponent-3))\n",
    "            difficulty_hexa = \"{0:x}\".format(data['bits'])\n",
    "            coefficient = difficulty_hexa[:2]\n",
    "            exponent = difficulty_hexa[:2]\n",
    "            diff_target.append(int(coefficient, 16) * pow(2, 8*(int(exponent, 16) -3)))\n",
    "\n",
    "\n",
    "            #Total BTC outputs in a block\n",
    "            total_btc_output = 0\n",
    "            for i in range(len(data['tx'])):\n",
    "                for j in range(len(data['tx'][i]['out'])):\n",
    "                    total_btc_output += data['tx'][i]['out'][j]['value']\n",
    "\n",
    "            total_btc_output_in_block.append(total_btc_output / pow(10,8))\n",
    "            \n",
    "            \n",
    "            df_ = pd.DataFrame({'creation_time':creation_time,\n",
    "                                'block_height':block_height,\n",
    "                                 'tx_number':tx_number,\n",
    "                                 'block_size':block_size,\n",
    "                                 'nonce':nonce,\n",
    "                                 'block_hash':block_hash,\n",
    "                                 'avg_tr_size':avg_tr_sizes,\n",
    "                                 'sum_tr_size':sum_tr_sizes,\n",
    "                                 'minin_fee':mining_fee,\n",
    "                                 'all_reward':all_reward,\n",
    "                                 'diff_target':diff_target,\n",
    "                                 'total_btc_output_per_block':total_btc_output_in_block,\n",
    "                                })\n",
    "            \n",
    "            df = pd.concat([df,df_], axis=0)\n",
    "            \n",
    "        \n",
    "            if k % 5 == 0:\n",
    "            \n",
    "                df.to_csv('Block_Features_23.csv', index=False)\n",
    "        \n",
    "        except ValueError: \n",
    "            \n",
    "            print('Decoding JSON has failed at day :',k,' block hash : ', n)\n",
    "        \n",
    "        except KeyError as error:\n",
    "            \n",
    "            print('KeyError at day:', k, 'block hash : ', n)\n",
    "\n",
    "            \n",
    "df.to_csv('Block_Features_23.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('Block_Features_23.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
